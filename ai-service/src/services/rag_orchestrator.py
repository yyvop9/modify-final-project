import asyncio
import logging
import aiohttp
import base64
import re
from io import BytesIO
from typing import List, Dict, Any, Optional, Set
from PIL import Image

from src.core.model_engine import model_engine
from src.services.quota_monitor import quota_monitor
from src.services.google_search_client import GoogleSearchClient

logger = logging.getLogger(__name__)

class AIOrchestrator:
    def __init__(self):
        self.engine = model_engine
        self.search_client = GoogleSearchClient()
        self.semaphore = asyncio.Semaphore(5)
        
        # âœ… ìœ ëª…ì¸/ì—°ì˜ˆì¸ ì´ë¦„ ëª©ë¡ (ë¹ ë¥¸ ì²´í¬ìš©)
        self.celebrity_names: Set[str] = {
            # ë‚¨ì ì•„í‹°ìŠ¤íŠ¸/ë˜í¼
            "ì§€ë“œë˜ê³¤", "GD", "ê¶Œì§€ìš©", "ì§€ë””",
            "ë¹…ë±…", "íƒœì–‘", "ëŒ€ì„±", "íƒ‘", "ìŠ¹ë¦¬",
            "ì§€ì½”", "ë°•ì¬ë²”", "ì‚¬ì´ë¨¼ë„ë¯¸ë‹‰", "ê·¸ë ˆì´", "ë¡œê¼¬",
            # ì—¬ì ì•„ì´ëŒ
            "ì¥ì›ì˜", "ì•ˆìœ ì§„", "ì´ì„œ", "ê°€ì„", "ë ˆì´",
            "ì¹´ë¦¬ë‚˜", "ìœˆí„°", "ì§€ì ¤", "ë‹ë‹",
            "ì œë‹ˆ", "ì§€ìˆ˜", "ë¡œì œ", "ë¦¬ì‚¬",
            "ë¯¼ì§€", "í•˜ë‹ˆ", "ë‹¤ë‹ˆì—˜", "í•´ë¦°", "í˜œì¸",
            "ì¹´ì¦ˆí•˜", "ì‚¬ì¿ ë¼", "ê¹€ì±„ì›", "í—ˆìœ¤ì§„", "í™ì€ì±„",
            "íƒœì—°", "ìœ¤ì•„", "ì„œí˜„", "í‹°íŒŒë‹ˆ", "ì œì‹œì¹´",
            "ë‚˜ì—°", "ì •ì—°", "ëª¨ëª¨", "ì‚¬ë‚˜", "ì§€íš¨", "ë¯¸ë‚˜", "ë‹¤í˜„", "ì±„ì˜", "ì¯”ìœ„",
            "ì•„ì´ë¦°", "ìŠ¬ê¸°", "ì›¬ë””", "ì¡°ì´", "ì˜ˆë¦¬",
            "ì¸„", "í¬ì§„", "í˜„ì§„", "ê³ ì›", "ê¹€ë¦½",
            # ì—¬ì ë°°ìš°
            "ì•„ì´ìœ ", "ìˆ˜ì§€", "ì†¡í˜œêµ", "ê¹€íƒœë¦¬", "í•œì†Œí¬", "ì „ì§€í˜„", "ê¹€ê³ ì€",
            "ì‹ ì„¸ê²½", "ë°•ë³´ì˜", "ì„¤í˜„", "ë°•ì‹ í˜œ", "ì†ì˜ˆì§„",
            "ê¹€ìœ ì •", "ê¹€ì†Œí˜„", "ì´ì„±ê²½", "ì„œì˜ˆì§€", "ë¬¸ê°€ì˜",
            # ë‚¨ì ì•„ì´ëŒ
            "ë·”", "ì •êµ­", "ì§€ë¯¼", "RM", "ìŠˆê°€", "ì§„", "ì œì´í™‰",
            "ë¯¼í˜¸", "íƒœë¯¼", "ì˜¨ìœ ", "í‚¤",
            "ë§ˆí¬", "ì¬í˜„", "ë„ì˜", "íƒœìš©", "ìŸˆë‹ˆ",
            "ë°©ì°¬", "ë¦¬ë…¸", "ì°½ë¹ˆ", "í•„ë¦­ìŠ¤", "ìŠ¹ë¯¼", "ì•„ì´ì—”",
            "ìˆ˜ë¹ˆ", "ì—°ì¤€", "ë²”ê·œ", "íƒœí˜„", "íœ´ë‹ì¹´ì´",
            # ë‚¨ì ë°°ìš°
            "ì°¨ì€ìš°", "ê³µìœ ", "í˜„ë¹ˆ", "ì´ì¢…ì„", "ë°•ì„œì¤€", "ì†¡ê°•", "ì´ë„í˜„",
            "ë°•ë³´ê²€", "ê¹€ìˆ˜í˜„", "ì´ë¯¼í˜¸", "ë‚¨ì£¼í˜", "ì„œê°•ì¤€",
            "ì†¡ì¤‘ê¸°", "ì´ì¤€ê¸°", "ì§€ì°½ìš±", "ë°•í˜•ì‹",
            # í•´ì™¸ ì—°ì˜ˆì¸
            "í…Œì¼ëŸ¬ìŠ¤ìœ„í”„íŠ¸", "ì•„ë¦¬ì•„ë‚˜ê·¸ë€ë°", "ë¹„ìš˜ì„¸", "ë¦¬í•œë‚˜",
            "ì  ë°ì´ì•„", "í‹°ëª¨ì‹œìƒ¬ë¼ë©”", "í†°í™€ëœë“œ"
        }
        
        # âœ… ì¼ë°˜ ëª…ì‚¬ ëª©ë¡ (ì´ë¦„ìœ¼ë¡œ ì˜¤ì¸í•˜ë©´ ì•ˆ ë˜ëŠ” ë‹¨ì–´ë“¤)
        self.common_words: Set[str] = {
            # ê³„ì ˆ
            "ê²¨ìš¸", "ì—¬ë¦„", "ë´„", "ê°€ì„", "ê²¨ìš¸ì—", "ì—¬ë¦„ì—", "ë´„ì—", "ê°€ì„ì—",
            # ì„±ë³„
            "ë‚¨ì", "ì—¬ì", "ë‚¨ì„±", "ì—¬ì„±", "ë‚¨ìì˜·", "ì—¬ìì˜·", "ë‚¨ì„±ë³µ", "ì—¬ì„±ë³µ",
            # ì˜ë¥˜ ì¢…ë¥˜
            "ì˜·", "ì½”íŠ¸", "íŒ¨ë”©", "ìì¼“", "ë°”ì§€", "ì¹˜ë§ˆ", "ì›í”¼ìŠ¤", "ì…”ì¸ ", "ë‹ˆíŠ¸",
            "ê°€ë””ê±´", "ë§¨íˆ¬ë§¨", "í›„ë“œ", "í‹°ì…”ì¸ ", "ì²­ë°”ì§€", "ìŠ¬ë™ìŠ¤", "ë ˆê¹…ìŠ¤",
            "ì •ì¥", "ìˆ˜íŠ¸", "ë¸”ë¼ìš°ìŠ¤", "ìŠ¤ì»¤íŠ¸", "ì¡°ë¼", "ë² ìŠ¤íŠ¸", "ì í¼",
            # ì¥ì†Œ/ìƒí™©
            "ìƒê°“ì§‘", "ì¥ë¡€ì‹", "ê²°í˜¼ì‹", "ì‹ì‚¬", "ì‹ì‚¬ìë¦¬", "ëª¨ì„", "íŒŒí‹°",
            "ì¶œê·¼", "í‡´ê·¼", "ë°ì´íŠ¸", "ì†Œê°œíŒ…", "ë©´ì ‘", "íšŒì‚¬", "í•™êµ",
            "êµíšŒ", "ì„±ë‹¹", "ì ˆ", "ëª…ì ˆ", "ì¶”ì„", "ì„¤ë‚ ", "í¬ë¦¬ìŠ¤ë§ˆìŠ¤",
            # í˜•ìš©ì‚¬/ë¶€ì‚¬
            "ê²©ì‹", "ê²©ì‹ìˆëŠ”", "ìºì£¼ì–¼", "í¸í•œ", "ë”°ëœ»í•œ", "ì‹œì›í•œ", "ê°€ë²¼ìš´",
            "ë¬´ê±°ìš´", "ê³ ê¸‰", "ì €ë ´í•œ", "ì˜ˆìœ", "ë©‹ì§„", "ì„¸ë ¨ëœ",
            # ë™ì‚¬/ì¡°ì‚¬ ì–´ê·¼
            "ì…ì„", "ì…ì„ë§Œí•œ", "ë§Œí•œ", "ì¶”ì²œ", "ì¶”ì²œí•´ì¤˜", "ë³´ì—¬ì¤˜", "ì°¾ì•„ì¤˜",
            "ì–´ìš¸ë¦¬ëŠ”", "ë§ëŠ”", "ì¢‹ì€", "ê´œì°®ì€",
            # ê¸°íƒ€ ì¼ë°˜ ëª…ì‚¬
            "ì–´ë¥¸", "ì–´ë¥¸ë“¤", "ì–´ë¥¸ë“¤ê³¼", "ë¶€ëª¨ë‹˜", "ì¹œêµ¬", "ë™ë£Œ", "ì„ ë°°", "í›„ë°°",
            "í•¨ê»˜", "í•¨ê»˜í•˜ëŠ”", "ê°™ì´", "í˜¼ì",
            "ìŠ¤íƒ€ì¼", "íŒ¨ì…˜", "ì½”ë””", "ë£©", "ì°©ì¥", "ì°¨ë¦¼",
            "ìƒì˜", "í•˜ì˜", "ì•„ìš°í„°", "ì´ë„ˆ", "ì‹ ë°œ", "ê°€ë°©", "ì•¡ì„¸ì„œë¦¬",
            # ì‹œê°„
            "ì˜¤ëŠ˜", "ë‚´ì¼", "ì£¼ë§", "í‰ì¼", "ì•„ì¹¨", "ì €ë…", "ë°¤",
            # ì¡°ì‚¬/ì–´ë¯¸ê°€ ë¶™ì€ í˜•íƒœ
            "ì—ì„œ", "ì—ì„œì˜", "ë•Œ", "ë•Œì˜", "ìš©", "ìœ„í•œ",
            # âœ… ì¶”ê°€: ìì£¼ ì˜¤ì¸ë˜ëŠ” ì¼ë°˜ ëª…ì‚¬
            "ìë™ì°¨", "ë¹„í–‰ê¸°", "ê¸°ì°¨", "ë²„ìŠ¤", "ì§€í•˜ì² ", "íƒì‹œ",
            "ê³µí•­", "ì—­", "í„°ë¯¸ë„", "ì •ë¥˜ì¥",
            "ì‚¬ì§„", "ì´ë¯¸ì§€", "ì˜ìƒ", "ë™ì˜ìƒ", "ë®¤ë¹„",
            "ì½˜ì„œíŠ¸", "ê³µì—°", "ë¬´ëŒ€", "í–‰ì‚¬", "ì´ë²¤íŠ¸",
            "ë¸Œëœë“œ", "ëª…í’ˆ", "ë¹ˆí‹°ì§€", "ë ˆíŠ¸ë¡œ", "í´ë˜ì‹",
            "íŠ¸ë Œë“œ", "ìœ í–‰", "ì¸ê¸°", "í•«í•œ", "ìš”ì¦˜"
        }
        
        # âœ… íŒ¨ì…˜ ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ
        self.fashion_context_keywords = [
            "íŒ¨ì…˜", "ìŠ¤íƒ€ì¼", "ì½”ë””", "ë£©", "ì°©ìš©", "ì˜ìƒ",
            "ê³µí•­", "ì‹œì‚¬íšŒ", "ë¬´ëŒ€", "í™”ë³´", "ì…ì€", "ì°©ì¥",
            "ì‚¬ë³µ", "ì¶œê·¼ë£©", "í‡´ê·¼ë£©", "ë°ì´íŠ¸ë£©"
        ]
        
        # âœ… í•œê¸€ ì´ë¦„ íŒ¨í„´ (2-3ê¸€ì, ì„±+ì´ë¦„)
        self.korean_name_pattern = re.compile(r'^[ê°€-í£]{2,3}$')

    async def _download_image(self, session: aiohttp.ClientSession, url: str) -> Optional[Image.Image]:
        async with self.semaphore:
            try:
                timeout = aiohttp.ClientTimeout(total=4)
                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                    "Referer": "https://www.google.com/"
                }
                async with session.get(url, headers=headers, timeout=timeout) as response:
                    if response.status == 200:
                        data = await response.read()
                        image = Image.open(BytesIO(data)).convert("RGB")
                        if image.width < 250 or image.height < 250: return None
                        return image
            except Exception as e:
                logger.debug(f"Image download failed: {url} - {e}")
                return None
        return None

    def _image_to_base64(self, image: Image.Image) -> str:
        try:
            buffered = BytesIO()
            image.save(buffered, format="JPEG", quality=95)
            img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
            return f"data:image/jpeg;base64,{img_str}"
        except Exception: return ""

    def _optimize_query_for_celebrity(self, user_query: str) -> str:
        """ì—°ì˜ˆì¸ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”"""
        stop_words = ["ì¶”ì²œí•´ì¤˜", "ë³´ì—¬ì¤˜", "ì°¾ì•„ì¤˜", "ì•Œë ¤ì¤˜", "ì–´ë•Œ", "ì¢€", "í•´ì¤˜"]
        
        words = user_query.split()
        keywords = []
        
        for w in words:
            clean_w = re.sub(r'(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì˜|ì—|ë¡œ|ìœ¼ë¡œ|ì™€|ê³¼|ë„|ë§Œ)$', '', w)
            if clean_w in stop_words or len(clean_w) < 2:
                continue
            keywords.append(clean_w)
        
        if not keywords:
            return user_query
            
        optimized = " ".join(keywords)
        logger.info(f"ğŸ” Query optimized: '{user_query}' -> '{optimized}'")
        return optimized

    def _get_scoring_context(self, query: str) -> str:
        if any(k in query for k in ["ê°€ë°©", "ì‹ ë°œ", "ì§€ê°‘", "ì•¡ì„¸ì„œë¦¬"]): 
            return "close up product shot"
        return "full body fashion style"

    def _normalize_score(self, raw_score: float) -> int:
        if raw_score < 0.15: return 0
        normalized = (raw_score - 0.15) * 450
        return int(min(max(normalized, 60), 99))

    async def process_external_rag(self, query: str) -> Dict[str, Any]:
        """ì™¸ë¶€ ì´ë¯¸ì§€ ê²€ìƒ‰ + VLM ë¶„ì„ (ì—°ì˜ˆì¸/ìœ ëª…ì¸ ê²€ìƒ‰ ì „ìš©)"""
        logger.info(f"ğŸŒ Processing EXTERNAL RAG: {query}")
        
        allowed, reason = quota_monitor.check_and_increment()
        if not allowed:
            logger.warning(f"âš ï¸ Quota exceeded: {reason}")
            return await self.process_internal_search(query)

        optimized_query = self._optimize_query_for_celebrity(query)
        
        logger.info(f"ğŸ” Searching Google Images: '{optimized_query}'")
        search_results = await self.search_client.search_images(
            optimized_query, num_results=15, start_index=1
        )
        
        if not search_results:
            logger.warning("âŒ No search results from Google")
            return await self.process_internal_search(query)
            
        logger.info(f"âœ… Found {len(search_results)} images")

        best_image = None
        candidates_data = []

        async with aiohttp.ClientSession() as session:
            tasks = [self._download_image(session, item['link']) for item in search_results]
            downloaded_images = await asyncio.gather(*tasks)

            scored_candidates = []
            clip_prompt = f"{optimized_query} {self._get_scoring_context(optimized_query)}"

            for i, img in enumerate(downloaded_images):
                if img:
                    base_score = self.engine.calculate_similarity(clip_prompt, img)
                    ratio_bonus = 0.05 if img.height > img.width else 0.0
                    final_score = base_score + ratio_bonus

                    if final_score > 0.18:
                        scored_candidates.append({
                            "image": img,
                            "url": search_results[i]['link'],
                            "raw_score": final_score,
                            "display_score": self._normalize_score(final_score)
                        })

            scored_candidates.sort(key=lambda x: x['raw_score'], reverse=True)
            top_candidates = scored_candidates[:4]

            if top_candidates:
                best_candidate = top_candidates[0]
                best_image = best_candidate['image']
                
                for cand in top_candidates:
                    candidates_data.append({
                        "image_base64": self._image_to_base64(cand['image']),
                        "score": cand['display_score']
                    })
                    
        logger.info(f"ğŸ“Š Valid candidates: {len(scored_candidates)}")

        if not best_image:
            logger.warning("âŒ No valid images after scoring")
            return await self.process_internal_search(query)

        summary = await self._analyze_image_with_vlm(best_image, query)
        final_data_uri = self._image_to_base64(best_image)

        vectors = {
            "bert": self.engine.generate_dual_embedding(summary)["bert"],
            "clip": self.engine.generate_image_embedding(best_image)["clip"]
        }

        return {
            "vectors": vectors,
            "search_path": "EXTERNAL",
            "strategy": "visual_rag_vlm",
            "ai_analysis": {
                "summary": summary,
                "reference_image": final_data_uri,
                "candidates": candidates_data
            },
            "description": summary,
            "ref_image": final_data_uri
        }

    async def analyze_specific_image(self, image_b64: str, query: str) -> str:
        try:
            if "base64," in image_b64:
                image_b64 = image_b64.split("base64,")[1]
            return await self._analyze_image_with_vlm(image_b64, query)
        except Exception:
            return "ì´ë¯¸ì§€ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

    async def _analyze_image_with_vlm(self, image_data: Any, query: str) -> str:
        """VLMì„ ì´ìš©í•œ ì´ë¯¸ì§€ ë¶„ì„"""
        try:
            if isinstance(image_data, Image.Image):
                img_b64 = self._image_to_base64(image_data).split(",")[1]
            else:
                img_b64 = image_data

            vlm_prompt = f"""
            ë‹¹ì‹ ì€ ì •ì§í•œ íŒ¨ì…˜ ì—ë””í„°ì…ë‹ˆë‹¤.
            **ì˜¤ì§ ì´ë¯¸ì§€ì— ì‹œê°ì ìœ¼ë¡œ ë³´ì´ëŠ” ê²ƒë§Œ** ì„¤ëª…í•˜ì„¸ìš”. 
            ì´ë¯¸ì§€ì— ì—†ëŠ” ë‚´ìš©(ìƒìƒ, ë°°ê²½ì§€ì‹, ì¶”ì¸¡)ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
            
            ì‚¬ìš©ì ì§ˆë¬¸: "{query}" (ì°¸ê³ ìš©ì¼ ë¿, ì‹¤ì œ ì´ë¯¸ì§€ ë‚´ìš©ì´ ìš°ì„ ì…ë‹ˆë‹¤.)
            
            [ë¶„ì„ ê°€ì´ë“œ]
            1. **íŠ¸ë Œë“œ ë¬´ë“œ**: ì´ë¯¸ì§€ì—ì„œ ëŠê»´ì§€ëŠ” ì‹¤ì œ ë¶„ìœ„ê¸°ë§Œ í•œ ì¤„ë¡œ ì‘ì„±.
            2. **ìŠ¤íƒ€ì¼ë§ í¬ì¸íŠ¸**: ëˆˆì— ë³´ì´ëŠ” ì˜·ì˜ ìƒ‰ìƒ, ì†Œì¬, í•ì„ êµ¬ì²´ì ìœ¼ë¡œ ë¬˜ì‚¬.
            3. **ì¶”ì²œ ì•„ì´í…œ**: ì´ ì‚¬ì§„ ì† ì¸ë¬¼ì´ ì°©ìš©í•œ ì•„ì´í…œê³¼ ìœ ì‚¬í•œ ì œí’ˆ ì¶”ì²œ.
            
            ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
            """
            return self.engine.generate_with_image(vlm_prompt, img_b64)
        except Exception as e:
            logger.error(f"VLM analysis failed: {e}")
            return "ë¶„ì„ ë¶ˆê°€"

    async def process_internal_search(self, query: str) -> Dict[str, Any]:
        """ë‚´ë¶€ í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ì¼ë°˜ ìƒí’ˆ ê²€ìƒ‰)"""
        logger.info(f"ğŸ“¦ Processing INTERNAL search: {query}")
        vectors = self.engine.generate_dual_embedding(query)
        return {
            "vectors": vectors,
            "search_path": "INTERNAL",
            "strategy": "internal_text",
            "ai_analysis": None,
            "description": f"'{query}' ë‚´ë¶€ ê²€ìƒ‰ ê²°ê³¼",
            "ref_image": None
        }

    def _extract_potential_names(self, query: str) -> List[str]:
        """
        ì¿¼ë¦¬ì—ì„œ ì ì¬ì ì¸ ì¸ë¬¼ ì´ë¦„ ì¶”ì¶œ
        - 2-3ê¸€ì í•œê¸€ ë‹¨ì–´ ì¤‘ ì¼ë°˜ ëª…ì‚¬ê°€ ì•„ë‹Œ ê²ƒ
        """
        # ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
        words = query.split()
        potential_names = []
        
        for word in words:
            # ì¡°ì‚¬ ì œê±°
            clean_word = re.sub(r'(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì˜|ì—|ë¡œ|ìœ¼ë¡œ|ì™€|ê³¼|ë„|ë§Œ|ì²˜ëŸ¼|ê°™ì€)$', '', word)
            
            # 2-3ê¸€ì í•œê¸€ì¸ì§€ ì²´í¬
            if not self.korean_name_pattern.match(clean_word):
                continue
            
            # ì¼ë°˜ ëª…ì‚¬ì¸ì§€ ì²´í¬
            if clean_word in self.common_words:
                continue
            
            # ì¼ë°˜ ëª…ì‚¬ì˜ ì¼ë¶€ì¸ì§€ ì²´í¬ (ì˜ˆ: "ê²¨ìš¸ì—" â†’ "ê²¨ìš¸")
            is_common = False
            for common in self.common_words:
                if clean_word in common or common in clean_word:
                    is_common = True
                    break
            
            if not is_common:
                potential_names.append(clean_word)
        
        return potential_names

    def _contains_celebrity(self, query: str) -> Optional[str]:
        """
        âœ… ìŠ¤ë§ˆíŠ¸í•œ ì—°ì˜ˆì¸/ìœ ëª…ì¸ ê°ì§€
        
        1ë‹¨ê³„: ì•Œë ¤ì§„ ì—°ì˜ˆì¸ ëª©ë¡ì—ì„œ ì²´í¬ (ë¹ ë¦„)
        2ë‹¨ê³„: í•œê¸€ ì´ë¦„ íŒ¨í„´(2-3ê¸€ì) ì¤‘ ì¼ë°˜ ëª…ì‚¬ ì•„ë‹Œ ê²ƒ ê°ì§€ (í—ˆê²½ì˜ ê°™ì€ ê²½ìš°)
        """
        query_normalized = query.replace(" ", "")
        
        # 1ë‹¨ê³„: ì•Œë ¤ì§„ ì—°ì˜ˆì¸ ì´ë¦„ ì²´í¬
        for name in self.celebrity_names:
            if name in query_normalized:
                logger.info(f"ğŸ¯ Known celebrity found: '{name}'")
                return name
        
        # 2ë‹¨ê³„: ì ì¬ì  ì´ë¦„ ì¶”ì¶œ (ì¼ë°˜ ëª…ì‚¬ ì œì™¸)
        potential_names = self._extract_potential_names(query)
        
        if potential_names:
            # íŒ¨ì…˜ ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
            has_fashion_context = any(k in query for k in self.fashion_context_keywords)
            
            if has_fashion_context:
                logger.info(f"ğŸ¯ Potential person name detected: {potential_names} with fashion context")
                return potential_names[0]
        
        return None

    async def determine_search_path(self, query: str) -> str:
        """
        âœ… ìŠ¤ë§ˆíŠ¸í•œ ê²€ìƒ‰ ê²½ë¡œ ê²°ì •
        
        EXTERNAL ì¡°ê±´ (Google ì´ë¯¸ì§€ ê²€ìƒ‰):
        - ì•Œë ¤ì§„ ì—°ì˜ˆì¸ ì´ë¦„ì´ í¬í•¨ëœ ê²½ìš°
        - ì ì¬ì  ì¸ë¬¼ ì´ë¦„(2-3ê¸€ì, ë¹„ì¼ë°˜ëª…ì‚¬) + íŒ¨ì…˜ í‚¤ì›Œë“œ ì¡°í•©
        
        INTERNAL ì¡°ê±´ (ë‚´ë¶€ DB ê²€ìƒ‰):
        - ì¼ë°˜ì ì¸ ìƒí’ˆ ê²€ìƒ‰
        - ìƒí™© ê¸°ë°˜ ì¶”ì²œ (ìƒê°“ì§‘, ê²°í˜¼ì‹ ë“±)
        
        ì˜ˆì‹œ:
        - "ì¥ì›ì˜ ê³µí•­íŒ¨ì…˜" â†’ EXTERNAL (ì•Œë ¤ì§„ ì—°ì˜ˆì¸)
        - "í—ˆê²½ì˜ íŒ¨ì…˜" â†’ EXTERNAL (2ê¸€ì ì´ë¦„ + íŒ¨ì…˜)
        - "ê¹€ì² ìˆ˜ ìŠ¤íƒ€ì¼" â†’ EXTERNAL (3ê¸€ì ì´ë¦„ + ìŠ¤íƒ€ì¼)
        - "ê²¨ìš¸ ë‚¨ìì˜· ì¶”ì²œ" â†’ INTERNAL (ì¼ë°˜ ê²€ìƒ‰)
        - "ìƒê°“ì§‘ ì˜· ì¶”ì²œ" â†’ INTERNAL (ìƒí™© ê¸°ë°˜)
        - "ê²©ì‹ìˆëŠ” ì‹ì‚¬ìë¦¬ ì˜·" â†’ INTERNAL (ìƒí™© ê¸°ë°˜)
        """
        
        # ì—°ì˜ˆì¸/ìœ ëª…ì¸ ì´ë¦„ ê°ì§€
        celebrity = self._contains_celebrity(query)
        
        if celebrity:
            logger.info(f"ğŸŒ Celebrity/Person search: '{celebrity}' in '{query}' -> EXTERNAL")
            return 'EXTERNAL'
        
        # ì¼ë°˜ ê²€ìƒ‰ì€ INTERNAL
        logger.info(f"ğŸ“¦ General product search: '{query}' -> INTERNAL")
        return 'INTERNAL'


rag_orchestrator = AIOrchestrator()