import os
import logging
import base64
import io
import threading
from typing import List, Optional, Dict, Union
from PIL import Image

# [AI Core]
import torch
from sentence_transformers import SentenceTransformer, util 
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ibm import ChatWatsonx
from langchain_core.messages import HumanMessage

logger = logging.getLogger(__name__)

# [ìƒìˆ˜ ì •ì˜]
BERT_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
CLIP_MODEL_NAME = "sentence-transformers/clip-ViT-B-32-multilingual-v1"
CLIP_VISION_MODEL_NAME = "sentence-transformers/clip-ViT-B-32"
VISION_MODEL_ID = "meta-llama/llama-3-2-11b-vision-instruct" 

class ModelEngine:
    """
    4-Model Hybrid Engine Singleton Class
    - Watsonx (VLM): Image Analysis (Writer)
    - BERT: Text Embedding (Retriever)
    - CLIP Text: Query to Vector (Scorer-Criteria)
    - CLIP Vision: Image to Vector (Scorer-Candidate)
    """
    _instance: Optional['ModelEngine'] = None
    _lock = threading.Lock() # Thread-safe initialization
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super(ModelEngine, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        # ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆë‹¤ë©´ ê±´ë„ˆëœ€ (Singleton ë³´ì¥)
        if hasattr(self, 'is_initialized') and self.is_initialized:
            return
            
        self.vision_model: Optional[ChatWatsonx] = None
        self.text_model: Optional[ChatWatsonx] = None
        self.bert_model: Optional[HuggingFaceEmbeddings] = None
        self.clip_text_model: Optional[SentenceTransformer] = None
        self.clip_vision_model: Optional[SentenceTransformer] = None
        
        self.project_id = os.getenv("WATSONX_PROJECT_ID")
        self.device = os.getenv("EMBEDDING_DEVICE", "cpu")
        self.is_initialized = False

    def initialize(self):
        """Lazy Loading Pattern: ì²« ìš”ì²­ ì‹œ ëª¨ë¸ ë¡œë“œ"""
        if self.is_initialized: return
        
        with self._lock:
            if self.is_initialized: return
            logger.info(f"ğŸš€ Initializing Hybrid Model Engine on [{self.device}]...")
            
            # 1. Watsonx (API ê¸°ë°˜ì´ë¼ ê°€ë²¼ì›€)
            self._init_watsonx()
            
            # 2. Local Models (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì£¼ì˜)
            try:
                logger.info("loading BERT...")
                self.bert_model = HuggingFaceEmbeddings(
                    model_name=BERT_MODEL_NAME,
                    model_kwargs={'device': self.device},
                    encode_kwargs={'normalize_embeddings': True}
                )
            except Exception as e: logger.error(f"âŒ BERT Failed: {e}")

            try:
                logger.info("loading CLIP Text...")
                self.clip_text_model = SentenceTransformer(CLIP_MODEL_NAME, device=self.device)
            except Exception as e: logger.error(f"âŒ CLIP Text Failed: {e}")

            try:
                logger.info("loading CLIP Vision...")
                self.clip_vision_model = SentenceTransformer(CLIP_VISION_MODEL_NAME, device=self.device)
            except Exception as e: logger.error(f"âŒ CLIP Vision Failed: {e}")

            self.is_initialized = True
            logger.info("âœ… All Models Initialized Successfully.")

    def _init_watsonx(self):
        try:
            api_key = os.getenv("WATSONX_API_KEY")
            url = os.getenv("WATSONX_URL", "https://us-south.ml.cloud.ibm.com")
            
            if api_key and self.project_id:
                # [ë¶„ì„ìš©] ì •í™•í•œ ë¬˜ì‚¬ë¥¼ ìœ„í•´ greedy decoding ì‚¬ìš©
                self.vision_model = ChatWatsonx(
                    model_id=VISION_MODEL_ID, url=url, apikey=api_key, project_id=self.project_id,
                    params={
                        "decoding_method": "greedy", 
                        "temperature": 0.0,
                        "max_new_tokens": 500,
                        "min_new_tokens": 10,
                        "repetition_penalty": 1.2
                    }
                )
                # [ì°½ì‘ìš©] í…ìŠ¤íŠ¸ ìƒì„±ì€ ì•½ê°„ì˜ ì°½ì˜ì„± í—ˆìš©
                self.text_model = ChatWatsonx(
                     model_id=VISION_MODEL_ID, url=url, apikey=api_key, project_id=self.project_id,
                     params={
                        "decoding_method": "sample",
                        "temperature": 0.3,
                        "max_new_tokens": 600,
                     }
                )
                logger.info(f"âœ… Watsonx Connected.")
            else:
                logger.warning("âš ï¸ Watsonx credentials missing.")
        except Exception as e: logger.error(f"âŒ Watsonx Init Failed: {e}")

    # ----------------------------------------------------------------
    # Core Logic Methods
    # ----------------------------------------------------------------

    def generate_embedding(self, text: str) -> List[float]:
        """
        BERT í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (768ì°¨ì›)
        - ìƒí’ˆ ë“±ë¡ ì‹œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ìš© ë²¡í„° ìƒì„±
        """
        if not self.bert_model:
            self.initialize()
        
        try:
            if self.bert_model:
                vector = self.bert_model.embed_query(text)
                return vector
            else:
                logger.warning("âš ï¸ BERT model not available, returning zeros")
                return [0.0] * 768
        except Exception as e:
            logger.error(f"âŒ BERT embedding failed: {e}")
            return [0.0] * 768

    def calculate_similarity(self, text: str, image: Image.Image) -> float:
        """Reranking Logic: CLIP Score Calculation"""
        if not self.clip_text_model or not self.clip_vision_model:
            self.initialize()
        try:
            # í…ìŠ¤íŠ¸ ì„ë² ë”© (ìºì‹± ê°€ëŠ¥í•˜ë‚˜, ì¿¼ë¦¬ê°€ ë§¤ë²ˆ ë‹¤ë¥´ë¯€ë¡œ ì‹¤ì‹œê°„ ìˆ˜í–‰)
            text_emb = self.clip_text_model.encode(text, convert_to_tensor=True)
            # ì´ë¯¸ì§€ ì„ë² ë”©
            img_emb = self.clip_vision_model.encode(image, convert_to_tensor=True)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            score = util.cos_sim(text_emb, img_emb).item()
            return score
        except Exception as e:
            logger.error(f"Similarity check failed: {e}")
            return 0.0

    def generate_dual_embedding(self, text: str) -> Dict[str, List[float]]:
        """BERT(ê²€ìƒ‰ìš©) + CLIP(ì‹œê°ì  ë§¤ì¹­ìš©) ë™ì‹œ ìƒì„±"""
        if not self.bert_model or not self.clip_text_model: self.initialize()
        
        result = {}
        # BERT
        if self.bert_model: 
            result["bert"] = self.bert_model.embed_query(text)
        else: 
            result["bert"] = [0.0] * 768
        
        # CLIP Text
        if self.clip_text_model:
            clip_emb = self.clip_text_model.encode(text)
            result["clip"] = clip_emb.tolist() if hasattr(clip_emb, "tolist") else clip_emb
        else: 
            result["clip"] = [0.0] * 512
            
        return result

    def generate_image_embedding(self, image_data: Union[str, Image.Image], use_yolo: bool = True) -> Dict[str, List[float]]:
        """
        ì´ë¯¸ì§€ -> CLIP Vector ë³€í™˜ (DB ì €ì¥ ë° ê²€ìƒ‰ìš©)
        
        Args:
            image_data: Base64 ë¬¸ìì—´ ë˜ëŠ” PIL Image
            use_yolo: YOLOë¡œ íŒ¨ì…˜ ì˜ì—­ í¬ë¡­ ì—¬ë¶€ (ê¸°ë³¸ True)
        """
        if not self.clip_vision_model: 
            self.initialize()
        try:
            pil_image = image_data
            if isinstance(image_data, str):
                # Base64 ì²˜ë¦¬
                if "base64," in image_data: 
                    image_data = image_data.split("base64,")[1]
                pil_image = Image.open(io.BytesIO(base64.b64decode(image_data)))
            
            # âœ… YOLOë¡œ íŒ¨ì…˜ ì˜ì—­ í¬ë¡­
            if use_yolo:
                try:
                    from src.core.yolo_detector import yolo_detector
                    cropped = yolo_detector.crop_fashion_regions(pil_image, target="full")
                    if cropped is not None:
                        pil_image = cropped
                        logger.info("âœ… YOLO: Fashion region cropped for CLIP")
                except ImportError:
                    logger.warning("âš ï¸ YOLO not available, using original image")
                except Exception as e:
                    logger.warning(f"âš ï¸ YOLO crop failed: {e}, using original image")
            
            if self.clip_vision_model:
                vector = self.clip_vision_model.encode(pil_image)
                return {"clip": vector.tolist() if hasattr(vector, "tolist") else list(vector)}
            else:
                logger.warning("âš ï¸ CLIP Vision model not available")
                return {"clip": [0.0] * 512}
        except Exception as e:
            logger.error(f"ğŸ–¼ï¸ Image Embedding Error: {e}")
            return {"clip": [0.0] * 512}

    def generate_with_image(self, text_prompt: str, image_b64: str) -> str:
        """VLM Inference"""
        if not self.vision_model: 
            self.initialize()
        try:
            if not self.vision_model:
                logger.error("âŒ Vision model not initialized")
                return "ì´ë¯¸ì§€ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (ëª¨ë¸ ë¯¸ì´ˆê¸°í™”)"
                
            message = HumanMessage(content=[
                {"type": "text", "text": text_prompt},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_b64}"}}
            ])
            return self.vision_model.invoke([message]).content
        except Exception as e:
            logger.error(f"Vision Error: {e}")
            return "ì´ë¯¸ì§€ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            
    def generate_text(self, prompt: str) -> str:
        if not self.text_model: 
            self.initialize()
        try:
            if not self.text_model:
                return ""
            return self.text_model.invoke(prompt).content
        except Exception as e:
            logger.error(f"Text Gen Error: {e}")
            return ""

    def generate_fashion_embeddings(self, image_data: Union[str, Image.Image]) -> Dict[str, List[float]]:
        """
        âœ… íŒ¨ì…˜ íŠ¹í™” ì„ë² ë”© - ì „ì‹ /ìƒì˜/í•˜ì˜ ê°ê° CLIP ë²¡í„° ìƒì„±
        
        Returns:
            {
                "full": [512 dim vector],
                "upper": [512 dim vector],
                "lower": [512 dim vector]
            }
        """
        if not self.clip_vision_model: 
            self.initialize()
            
        result = {
            "full": [0.0] * 512,
            "upper": [0.0] * 512,
            "lower": [0.0] * 512
        }
        
        try:
            pil_image = image_data
            if isinstance(image_data, str):
                if "base64," in image_data: 
                    image_data = image_data.split("base64,")[1]
                pil_image = Image.open(io.BytesIO(base64.b64decode(image_data)))
            
            # YOLOë¡œ ê° ì˜ì—­ ì¶”ì¶œ
            try:
                from src.core.yolo_detector import yolo_detector
                fashion_regions = yolo_detector.extract_fashion_features(pil_image)
                
                # ê° ì˜ì—­ CLIP ë²¡í„° ìƒì„±
                for region_name, region_img in fashion_regions.items():
                    if region_img is not None and self.clip_vision_model:
                        vector = self.clip_vision_model.encode(region_img)
                        result[region_name] = vector.tolist() if hasattr(vector, "tolist") else list(vector)
                        logger.info(f"âœ… Generated CLIP vector for '{region_name}': {len(result[region_name])} dims")
                        
            except ImportError:
                logger.warning("âš ï¸ YOLO not available, using full image only")
                if self.clip_vision_model:
                    vector = self.clip_vision_model.encode(pil_image)
                    result["full"] = vector.tolist() if hasattr(vector, "tolist") else list(vector)
            except Exception as e:
                logger.warning(f"âš ï¸ YOLO failed: {e}, using full image")
                if self.clip_vision_model:
                    vector = self.clip_vision_model.encode(pil_image)
                    result["full"] = vector.tolist() if hasattr(vector, "tolist") else list(vector)
                    
        except Exception as e:
            logger.error(f"âŒ Fashion embedding failed: {e}")
            
        return result

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
model_engine = ModelEngine()